{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGENDA:\n",
    "\n",
    "\n",
    "## Created Twitter Dev account and retreived LIVE tweets \n",
    "## Scrapping the tweets from twitter\n",
    "## Cleaned the tweets of any stopwords, #, @, lemmetization, punctuations, smiley's, repetitions, etc.\n",
    "## Finally separate CSV files were created for each political party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tegbir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tegbir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'preprocessor' has no attribute 'clean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-aea4a4697a0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;31m#calling the mainfunction using our keywords and the ouput filepaths\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m \u001b[0mwrite_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndp_keywords\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mndp_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[0mwrite_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConservative_keywords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconservative_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[0mwrite_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGreenParty_keywords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgreen_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-aea4a4697a0b>\u001b[0m in \u001b[0;36mwrite_tweets\u001b[1;34m(keyword, file)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;31m#method for basic preprocessing:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m             \u001b[0mclean_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'preprocessor' has no attribute 'clean'"
     ]
    }
   ],
   "source": [
    "########################################Sentiment Analysis on Political parties using Twitter Data #########################\n",
    "##############                                   Author : Tegbir Singh Sidhu, Rupinder Sandhu                        ########\n",
    "##############                                                                                                      ########\n",
    "#############                                   Date: 2nd December, 2019                                            ########\n",
    "############################################################################################################################\n",
    "\n",
    "import os\n",
    "import tweepy\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import preprocessor as p\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "#########################################################################################################\n",
    "\n",
    "#passing authentication keys for the twitter API\n",
    "\n",
    "consumer_key = 'O50ppVwfOVht4F0WkmDjp9tcF'\n",
    "consumer_secret = '4t9BSNG3XWyJOQM1TvHWcQT1aDg5VLHLRWHbnkr7J4VKykFxaZ'\n",
    "access_key= '1048701354829631493-nvNsvEG2Jm1Dnv6JgFABqSr2MlByD1'\n",
    "access_secret = 'Rbc1XohWaBDYYP7SMQSPIanRtEXFoJnM4vhKHOHan0zbK'\n",
    "\n",
    "#supplying the keys into OAuthHandler of tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "#Designing the CSV files\n",
    "#creating paths to store the received data from twitter\n",
    "ndp_tweets = \"C:/Users/Tegbir/Desktop/MyPython/ndp.csv\"\n",
    "liberal_tweets = \"C:/Users/Tegbir/Desktop/MyPython/liberal.csv\"\n",
    "conservative_tweets = \"C:/Users/Tegbir/Desktop/MyPython/conservative.csv\"\n",
    "green_tweets = \"C:/Users/Tegbir/Desktop/MyPython/green.csv\"\n",
    "Bloc_tweets = \"C:/Users/Tegbir/Desktop/MyPython/bloc.csv\"\n",
    "Peoplesparty_tweets = \"C:/Users/Tegbir/Desktop/MyPython/peoplesparty.csv\"\n",
    "\n",
    "#columns of the csv file\n",
    "fields = ['id', 'created_at', 'source', 'original_text','clean_text', 'sentiment','polarity','subjectivity', 'lang',\n",
    "'favorite_count', 'retweet_count', 'original_author',   'possibly_sensitive', 'hashtags',\n",
    "'user_mentions']\n",
    "\n",
    "#################################################################################################################\n",
    "\n",
    "#Defining data sets for tranformations\n",
    "#HappyEmoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "\n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "#making bigregex for replacing happy emotions with word happy\n",
    "big_regex_happy = re.compile('|'.join(map(re.escape, emoticons_happy)))\n",
    "big_regex_sad = re.compile('|'.join(map(re.escape, emoticons_sad)))\n",
    "\n",
    "#combine sad and happy emoticons\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "Extra_Symbols =\"*/.\"\n",
    "\n",
    "#calling the WordNetLemmatizer method from the corpus of NLTK package.\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "def TweetsClean (tweet):\n",
    "    \n",
    "    #this will get the stopwords from NLTK.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    #need to remove : and Ä¶ \n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r',Ä¶', '', tweet)\n",
    "    tweet = re.sub(r'â€™','', tweet)\n",
    "    tweet = re.sub(r'[â€¦]','', tweet)\n",
    "    tweet = re.sub(r'[â€¦\"\\~``*.]','', tweet)\n",
    "    tweet = tweet.replace(\"'\",\"\")\n",
    "    tweet = tweet.replace('\"',\"\")\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "    \n",
    "    #substitute the emoticons with their associated feeling:\n",
    "    tweet = big_regex_happy.sub('happy', tweet)\n",
    "    tweet = big_regex_sad.sub('sad', tweet)\n",
    "    \n",
    "    #removing the emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "    \n",
    "    #tokenizing the text in the twitter data\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "    \n",
    "    #filtering using stopwords and punctuations\n",
    "    #filter using NLTK library and append it to a string\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words and string.punctuation]\n",
    "    filtered_tweet = []\n",
    "    \n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words and w not in string.punctuation and w not in Extra_Symbols:\n",
    "            filtered_tweet.append(wordnet_lemmatizer.lemmatize(w, pos='a'))\n",
    "    return' '.join(filtered_tweet)\n",
    "    \n",
    "###############################################################################################################\n",
    "#Now in this part we will write the output to the csv files\n",
    "\n",
    "def write_tweets(keyword, file):\n",
    "    if os.path.exists(file):\n",
    "        df = pd.read_csv(file, header = 0)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns = fields)\n",
    "    for page in tweepy.Cursor(api.search, q=keyword, count = 200, include_rts = False).pages(50):\n",
    "        \n",
    "        #response from twitter is in json format\n",
    "        for status in page:\n",
    "            new_entry = []\n",
    "            status = status._json\n",
    "            if status['lang'] != 'en':\n",
    "                continue\n",
    "\n",
    "            #Now we will call the methods\n",
    "\n",
    "            #method for basic preprocessing:\n",
    "            clean_text = p.clean(status['text'])\n",
    "            \n",
    "            \n",
    "            #calling the data cleaning method:\n",
    "            filtered_tweet = TweetsClean(clean_text)\n",
    "            filtered_tweet.lower()\n",
    "            \n",
    "            #using textblob to perform sentiment check.\n",
    "            blob = TextBlob(filtered_tweet.lower())\n",
    "            Sentiment = blob.sentiment\n",
    "            polarity = Sentiment.polarity\n",
    "            subjectivity = Sentiment.subjectivity\n",
    "            new_entry += [status['id'], status['created_at'],status['source'],status['text'], \n",
    "                          filtered_tweet, Sentiment, polarity, subjectivity, status['lang'], \n",
    "                          status['favorite_count'], status['retweet_count']]\n",
    "            new_entry.append(status['user']['screen_name'])\n",
    "            try:\n",
    "                is_sensitive = status['possibly_sensitive']\n",
    "            except KeyError:\n",
    "                is_sensitive = None\n",
    "            new_entry.append(is_sensitive)\n",
    "            \n",
    "            #checking for hashtags\n",
    "            hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n",
    "            new_entry.append(hashtags)\n",
    "            #checking for mentions\n",
    "            mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n",
    "            new_entry.append(mentions)\n",
    "            \n",
    "            single_tweet_df = pd.DataFrame([new_entry], columns=COLS)\n",
    "            df = df.append(single_tweet_df, ignore_index=True)\n",
    "            #writing to a datafram csvFile using the utf-8 encoding\n",
    "            csvFile = open(file, 'a' ,encoding='utf-8')\n",
    "    #writing the dataframe to csvfiels \n",
    "    df.to_csv(csvFile, mode='a', columns=fields, index=False, encoding=\"utf-8\")\n",
    "    \n",
    "######################################################################################################################\n",
    "    \n",
    "#declaring the keywords for our twitter data\n",
    "ndp_keywords = '#ndp OR #jagmeetsingh OR #ptndp OR #theJagmeetSingh '\n",
    "Conservative_keywords = '#AndrewScheer OR #conservative OR #ConservativeParty OR #cpc OR #pttory'\n",
    "GreenParty_keywords = '#greenparty OR #greenparty OR #gpc OR #ptgreen'\n",
    "Liberal_keywords = '#liberalparty OR #justintrudeau OR #lpc OR #ptlib'\n",
    "Bloc_keywords = '#bloc OR #blocQuebec OR #ptbloc'\n",
    "PeoplesParty_keywords = '#Peoplesparty OR #PeoplesPartyCanada'\n",
    "\n",
    "#################################################################################################################\n",
    "\n",
    "#calling the mainfunction using our keywords and the ouput filepaths\n",
    "write_tweets(ndp_keywords,  ndp_tweets)\n",
    "write_tweets(Conservative_keywords, conservative_tweets)\n",
    "write_tweets(GreenParty_keywords, green_tweets)\n",
    "write_tweets(Liberal_keywords, liberal_tweets)\n",
    "write_tweets(Bloc_keywords, Bloc_tweets)\n",
    "write_tweets(PeoplesParty_keywords, Peoplesparty_tweets)\n",
    "\n",
    "#########################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
